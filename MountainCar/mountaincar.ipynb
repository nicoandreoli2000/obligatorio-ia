{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MountainCarEnv import MountainCarEnv\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MountainCarEnv(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_space = np.linspace(-5, 5, 10)\n",
    "vel_space = np.linspace(-3, 3, 2)\n",
    "\n",
    "Q = np.random.uniform(0, 0, (len(pos_space), len(vel_space), env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state[0], state[1]])\n",
    "    return action\n",
    "\n",
    "def get_state(obs):\n",
    "    pos, vel = obs\n",
    "    pos_bin = np.digitize(pos, pos_space)\n",
    "    vel_bin = np.digitize(vel, vel_space)\n",
    "    return pos_bin, vel_bin\n",
    "\n",
    "def run_mountain_car(explore_per = 0.5, learn=True, alpha=0.9, gamma=0.99, Q: np.ndarray = None):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    totalReward = 0\n",
    "    while not done:\n",
    "        state = get_state(obs)\n",
    "        Q_state = Q[state[0], state[1]]\n",
    "        action = epsilon_greedy_policy(state, Q, explore_per)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = get_state(obs)\n",
    "        new_Q_state = Q[new_state[0], new_state[1]]\n",
    "        totalReward += reward\n",
    "        if learn:\n",
    "            Q[state[0], state[1], action] = Q_state[action] + alpha * (reward + gamma * np.max(new_Q_state) - Q_state[action])\n",
    "    return totalReward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Q = []\n",
    "max_Q = []\n",
    "learning_rewards = []\n",
    "testing_rewards = []\n",
    "\n",
    "# Learn\n",
    "for i in range(100):\n",
    "    totalReward = run_mountain_car(explore_per=0.5, learn=True, Q=Q, alpha=0.1)\n",
    "    learning_rewards.append(totalReward)\n",
    "\n",
    "    mean_Q.append(np.mean(Q))\n",
    "    max_Q.append(np.max(Q))\n",
    "\n",
    "# Test\n",
    "for i in range(10):\n",
    "    totalReward = run_mountain_car(explore_per=0, learn=False, Q=Q)\n",
    "    testing_rewards.append(totalReward)\n",
    "\n",
    "# Plot\n",
    "plt.plot(list(range(1, len(max_Q) + 1)), max_Q, color='red', label='Valor mÃ¡ximo de Q')\n",
    "plt.plot(list(range(1, len(mean_Q) + 1)), mean_Q, color='blue', label='Valor promedio de Q')\n",
    "plt.xlabel('Nro de partida', fontsize=20)\n",
    "plt.ylabel('Valor de Q', fontsize=20)\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 10)\n",
    "plt.rcParams['xtick.labelsize'] = 'x-large'\n",
    "plt.rcParams['ytick.labelsize'] = 'x-large'\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pygame\n",
    "\n",
    "# # Initialize Pygame\n",
    "# pygame.init()\n",
    "\n",
    "# # Create a Pygame display surface\n",
    "# display = pygame.display.set_mode((600, 400))\n",
    "\n",
    "# # Your game or visualization code goes here\n",
    "\n",
    "# # Close the existing Pygame window\n",
    "# pygame.display.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = env.reset()\n",
    "# print(obs)\n",
    "# done = False\n",
    "# while not done:\n",
    "#     state = get_state(obs)\n",
    "#     action = epsilon_greedy_policy(state, Q, 0.5)\n",
    "#     obs, reward, done, _ = env.step(action)\n",
    "#     print('->', state, action, reward, obs, done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
